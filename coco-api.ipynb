{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bce4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO \n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "def calculate_coco_map(gt_json_path, pred_json_path):\n",
    "    \"\"\"\n",
    "    Calculate the mAP score using COCO API.\n",
    "    Args:\n",
    "        gt_json_path (str): Path to ground truth json file (COCO format).\n",
    "        pred_json_path (str): Path to prediction json file (COCO format).\n",
    "    Returns:\n",
    "        float: mAP score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load ground truth data\n",
    "    coco_gt = COCO(gt_json_path)\n",
    "    \n",
    "    # Load prediction data\n",
    "    coco_pred = coco_gt.loadRes(pred_json_path)\n",
    "\n",
    "    # Set up evaluation (use 'iou' method for mAP)\n",
    "    coco_eval = COCOeval(coco_gt, coco_pred, iouType='bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()  # This prints the mAP and other stats\n",
    "    \n",
    "    return coco_eval  # Index 0 corresponds to mAP @ IoU=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c56b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_detection_errors(coco_eval, iou_thresh_index=0):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    print(\"\\n🔍 Detection Error Analysis:\\n\")\n",
    "\n",
    "    false_positive_imgs = defaultdict(list)\n",
    "    misaligned_imgs = defaultdict(list)\n",
    "    seen = set()  # Track (image_id, category_id) pairs already processed\n",
    "\n",
    "    for eval_img in coco_eval.evalImgs:\n",
    "        if eval_img is None:\n",
    "            continue\n",
    "\n",
    "        image_id = eval_img['image_id']\n",
    "        category_id = eval_img['category_id']\n",
    "        key = (image_id, category_id)\n",
    "\n",
    "        # Prevent duplicate processing\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "\n",
    "        file_name = image_id\n",
    "        dt_matches = eval_img['dtMatches'][iou_thresh_index]\n",
    "        dt_scores = eval_img['dtScores']\n",
    "        gt_ids = eval_img['gtIds']\n",
    "        dt_ids = eval_img['dtIds']\n",
    "\n",
    "        # --- Case 1: No GT but detections (likely false positive) ---\n",
    "        if len(gt_ids) == 0 and len(dt_ids) > 0:\n",
    "            false_positive_imgs[file_name].append({\n",
    "                \"category_id\": category_id,\n",
    "                \"scores\": dt_scores\n",
    "            })\n",
    "\n",
    "        # --- Case 2: Has GT but predictions are not matched ---\n",
    "        if len(gt_ids) > 0 and len(dt_ids) > 0:\n",
    "            unmatched = [score for m, score in zip(dt_matches, dt_scores) if m == 0]\n",
    "            if unmatched:\n",
    "                misaligned_imgs[file_name].append({\n",
    "                    \"category_id\": category_id,\n",
    "                    \"unmatched_scores\": unmatched\n",
    "                })\n",
    "\n",
    "    # Print likely false positives\n",
    "    print(\"❌ False Positives (No GT but predictions):\")\n",
    "    for img, preds in false_positive_imgs.items():\n",
    "        for pred in preds:\n",
    "            print(f\"Image: {img}\")\n",
    "            print(f\" - Category: {pred['category_id']}\")\n",
    "            print(f\" - Confidence Scores: {pred['scores']}\")\n",
    "            print(f\" - Note: No ground truth, but detection exists.\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    # Print misaligned predictions\n",
    "    print(\"\\n⚠️ Misaligned Predictions (With GT but unmatched detections):\")\n",
    "    for img, preds in misaligned_imgs.items():\n",
    "        for pred in preds:\n",
    "            print(f\"Image: {img}\")\n",
    "            print(f\" - Category: {pred['category_id']}\")\n",
    "            print(f\" - Unmatched Scores: {pred['unmatched_scores']}\")\n",
    "            print(f\" - Note: GT exists but detection did not match (low IoU or misclass).\")\n",
    "            print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ae0ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.05s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.358\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.621\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.377\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.075\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.217\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.390\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.374\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.425\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.426\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.070\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.294\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.460\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'analyze_detection_errors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m gt_json_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/BTXRD-Dataset/BTXRD-Yolo/val/labels/coco_annotations.json\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your actual prediction JSON path\u001b[39;00m\n\u001b[0;32m      7\u001b[0m coco_eval \u001b[38;5;241m=\u001b[39m calculate_coco_map(gt_json_path, pred_json_path)\n\u001b[1;32m----> 8\u001b[0m \u001b[43manalyze_detection_errors\u001b[49m(coco_eval)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'analyze_detection_errors' is not defined"
     ]
    }
   ],
   "source": [
    "#pred_json_path = 'coco_ensemble_predictions.json'  # Replace with your actual ground truth JSON path\n",
    "#pred_json_path = 'D:/BTXRD-Code/Object-Detection/runs/detect/val5/predictions.json'  # Replace with your actual prediction JSON path\n",
    "#pred_json_path = 'fused_output.json'  \n",
    "pred_json_path = 'D:/BTXRD-Dataset/BTXRD-Yolo/val/labels/detect.json'\n",
    "gt_json_path = 'D:/BTXRD-Dataset/BTXRD-Yolo/val/labels/coco_annotations.json'  # Replace with your actual prediction JSON path\n",
    "\n",
    "coco_eval = calculate_coco_map(gt_json_path, pred_json_path)\n",
    "analyze_detection_errors(coco_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b5da2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG000024\n",
      "IMG000044\n",
      "IMG000049\n",
      "IMG000077\n",
      "IMG000108\n",
      "IMG000114\n",
      "IMG000129\n",
      "IMG000199\n",
      "IMG000221\n",
      "IMG000244\n",
      "IMG000260\n",
      "IMG000278\n",
      "IMG000317\n",
      "IMG000340\n",
      "IMG000359\n",
      "IMG000366\n",
      "IMG000411\n",
      "IMG000436\n",
      "IMG000439\n",
      "IMG000452\n",
      "IMG000495\n",
      "IMG000498\n",
      "IMG000527\n",
      "IMG000590\n",
      "IMG000627\n",
      "IMG000629\n",
      "IMG000650\n",
      "IMG000666\n",
      "IMG000667\n",
      "IMG000680\n",
      "IMG000702\n",
      "IMG000708\n",
      "IMG000712\n",
      "IMG000723\n",
      "IMG000740\n",
      "IMG000786\n",
      "IMG000819\n",
      "IMG000827\n",
      "IMG000848\n",
      "IMG000865\n",
      "IMG000870\n",
      "IMG000891\n",
      "IMG000922\n",
      "IMG000990\n",
      "IMG001024\n",
      "IMG001027\n",
      "IMG001125\n",
      "IMG001249\n",
      "IMG001256\n",
      "IMG001304\n",
      "IMG001379\n",
      "IMG001414\n",
      "IMG001439\n",
      "IMG001483\n",
      "IMG001513\n",
      "IMG001580\n",
      "IMG001671\n",
      "IMG001717\n",
      "IMG001741\n",
      "IMG001758\n",
      "IMG001769\n",
      "IMG001782\n",
      "IMG001858\n",
      "IMG001866\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# File paths\n",
    "detect_path = 'D:/BTXRD-Dataset/BTXRD-Yolo/val/labels/detect.json'\n",
    "coco_path = r\"D:\\BTXRD-Dataset\\BTXRD-Yolo\\val\\labels\\coco_annotations.json\"\n",
    "\n",
    "# Load detect.json\n",
    "with open(detect_path, 'r') as f:\n",
    "    detect_data = json.load(f)\n",
    "\n",
    "# Load coco_annotations.json\n",
    "with open(coco_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Get image_ids from COCO annotations\n",
    "coco_image_ids = set(ann[\"image_id\"] for ann in coco_data[\"annotations\"])\n",
    "\n",
    "# Filter detections where image_id exists in COCO annotations\n",
    "filtered_image_ids = [img_id for img_id in coco_image_ids if img_id not in {det[\"image_id\"] for det in detect_data}]\n",
    "\n",
    "\n",
    "# Print result\n",
    "for img_id in sorted(filtered_image_ids):\n",
    "    print(img_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
